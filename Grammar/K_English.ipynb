{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KgSNH8JWddAo"
      },
      "outputs": [],
      "source": [
        "\n",
        "import numpy as np \n",
        "import pandas as pd \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "clmndWdUdj8C"
      },
      "outputs": [],
      "source": [
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "sns.set(style='darkgrid', font_scale=1)\n",
        "data = pd.read_csv('train.csv')\n",
        "\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "train_df, test_df,y_train,y_test = train_test_split(data, data, test_size = 0.3,random_state=50)\n",
        "\n",
        "\n",
        "train_df = pd.read_csv('train.csv')\n",
        "test_df = pd.read_csv('test.csv')\n",
        "\n",
        "\n",
        "# Checking for null values\n",
        "train_df.isnull().sum() ,train_df.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nv9n-OIDdnda"
      },
      "outputs": [],
      "source": [
        "bert_path = '/huggingface-bert-variants/bert-base-uncased/bert-base-uncased'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4swukblRdq86"
      },
      "outputs": [],
      "source": [
        "targets = ['cohesion', 'syntax', 'vocabulary', 'phraseology', 'grammar', 'conventions']\n",
        "\n",
        "fig, axes = plt.subplots(2, len(targets)//2, figsize=(15,6))\n",
        "\n",
        "for i, target in enumerate(targets):\n",
        "    ax = axes.flat[i]\n",
        "    sns.histplot(x=target, data=train_df, linewidth=1.25, alpha=1, ax=ax, zorder=2)\n",
        "    ax.set_title(target)\n",
        "    ax.set(xlabel=None, ylabel=None)\n",
        "    \n",
        "fig.suptitle('Output Variables')\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DM8mLfsQduOh"
      },
      "outputs": [],
      "source": [
        "k = len(train_df.columns) #number of variables for heatmap\n",
        "f,ax = plt.subplots(figsize=(11, 11))\n",
        "cols = train_df.corr().abs().nlargest(k, 'syntax')['syntax'].index\n",
        "cm = np.corrcoef(train_df[cols].values.T)\n",
        "sns.set(font_scale=1.25)\n",
        "hm = sns.heatmap(cm, cbar=True, annot=True, square=True, fmt='.2f', annot_kws={'size': 10}, yticklabels=cols.values, xticklabels=cols.values)\n",
        "plt.show()\n",
        "cols = cols.drop('cohesion')\n",
        "cols"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lnc9a-wpdw9C"
      },
      "outputs": [],
      "source": [
        "# Merging Train and Test Data\n",
        "ntrain = train_df.shape[0]\n",
        "ntest = test_df.shape[0]\n",
        "all_data = pd.concat((train_df, test_df)).reset_index(drop=True)\n",
        "all_data.drop(['text_id'], axis=1, inplace=True)\n",
        "print(\"all_data size is : {}\".format(all_data.shape))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h9u7fCFQdzpq"
      },
      "outputs": [],
      "source": [
        "import string\n",
        "from nltk.corpus import stopwords\n",
        "PUNCT_TO_REMOVE = string.punctuation\n",
        "STOPWORDS = set(stopwords.words('english'))\n",
        "from nltk.stem import PorterStemmer\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "import re\n",
        "\n",
        "# Create an instance of a PorterStemmer object\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "def preprocess(text) : \n",
        "    #Using cased model so not lowering \n",
        "    text = text.lower()\n",
        "    text = re.sub(r'https?://\\S+|www\\.\\S+','', text)\n",
        "    text = re.sub(r'@[0-9a-zA-Z]*\\W+',' ' , text)\n",
        "    #text = re.sub(r'\\.','[SEP]' , text)\n",
        "    \n",
        "    #text = re.sub(r'\\W+', ' ', text)\n",
        "    text = re.sub(r'\\d+', ' ', text)\n",
        "    text = re.sub(r'\\#', ' ', text)\n",
        "    text = re.sub(r'\\'', ' ', text)\n",
        "        \n",
        "    list_text = text.split()\n",
        "    text = ' '.join(list_text[:512])\n",
        "    return text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YscsrTJTd2v6"
      },
      "outputs": [],
      "source": [
        "test = 'Our \\'fffff\\'Deeds are the . Reason of @insta this #earthquake M'\n",
        "print(preprocess(test))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ihpHiRUAd5DS"
      },
      "outputs": [],
      "source": [
        "all_data['full_text'] = all_data['full_text'].apply(lambda text : preprocess(text))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WzVOfsftd7BC"
      },
      "outputs": [],
      "source": [
        "train_data = all_data[:ntrain].copy()\n",
        "test_data = all_data[ntrain:]\n",
        "\n",
        "train_data.shape , test_data.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ciaaLMSrd9li"
      },
      "outputs": [],
      "source": [
        "BATCH_SIZE = 6\n",
        "\n",
        "MAX_LEN = max(len(x.split()) for x in all_data['full_text'])\n",
        "print(MAX_LEN)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LR_CIgE_d_1L"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from transformers import BertTokenizer , TFBertModel \n",
        "\n",
        "AUTO = tf.data.experimental.AUTOTUNE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3VFSpNeseD_j"
      },
      "outputs": [],
      "source": [
        "tokenizer = BertTokenizer.from_pretrained(bert_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XhGLqRdKeHaK"
      },
      "outputs": [],
      "source": [
        "def encode(input_text):\n",
        "    inputs = tokenizer.batch_encode_plus(input_text,padding='max_length',max_length=MAX_LEN, truncation=True)\n",
        "    return inputs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "60dZ9JibeJ-b"
      },
      "outputs": [],
      "source": [
        "train_input = encode(train_data['full_text'].values.tolist())['input_ids']\n",
        "\n",
        "train_data_ds = (\n",
        "    tf.data.Dataset\n",
        "    .from_tensor_slices((train_input,train_data.drop('full_text', axis = 1)))\n",
        "    .repeat()\n",
        "    .batch(BATCH_SIZE)\n",
        "    .prefetch(AUTO)\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VdYWbpsbeMQa"
      },
      "outputs": [],
      "source": [
        "testing_input = encode(test_data.full_text.values.tolist())['input_ids']\n",
        "\n",
        "test_dataset = (\n",
        "    tf.data.Dataset\n",
        "    .from_tensor_slices(testing_input)\n",
        "    .batch(BATCH_SIZE)\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rp47cTDBePCy"
      },
      "outputs": [],
      "source": [
        "# Custom error function MCRMSE : column wise root mean squared eoor\n",
        "def MCRMSE(y_true, y_pred):\n",
        "    colwise_mse = tf.reduce_mean(tf.square(y_true - y_pred), axis=1)\n",
        "    return tf.reduce_mean(tf.sqrt(colwise_mse), axis=-1, keepdims=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YbrIVUVPeRyC"
      },
      "outputs": [],
      "source": [
        "def create_model():\n",
        "    bert_encoder = TFBertModel.from_pretrained(bert_path )\n",
        "    input_word_ids = tf.keras.Input(shape=(MAX_LEN,), dtype=tf.int32, name=\"input_word_ids\")\n",
        "\n",
        "    embedding = bert_encoder(input_word_ids)[0]\n",
        "    x = tf.keras.layers.GlobalAveragePooling1D()(embedding)\n",
        "    x = tf.keras.layers.LayerNormalization()(x)\n",
        "    #Output layer without activation function because regression task\n",
        "    output = tf.keras.layers.Dense(6,)(x)\n",
        "\n",
        "    model = tf.keras.models.Model(inputs=input_word_ids, outputs=output)\n",
        "    model.compile(optimizer=tf.keras.optimizers.Adam(1e-5), loss=MCRMSE\n",
        "                  , metrics=MCRMSE)\n",
        "\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CtBuItZEeUQL"
      },
      "outputs": [],
      "source": [
        "model= create_model()\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jy1Hk5PNeWpK"
      },
      "outputs": [],
      "source": [
        "import  gc\n",
        "gc.collect()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g8zthQfEeZDK"
      },
      "outputs": [],
      "source": [
        "BATCH_SIZE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UVvoKznpebeC"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "\n",
        "callback = tf.keras.callbacks.EarlyStopping(monitor='MCRMSE', patience = 2 ,restore_best_weights=True)\n",
        "\n",
        "history = model.fit(\n",
        "                    train_data_ds, \n",
        "                    steps_per_epoch= train_data.shape[0]//BATCH_SIZE,\n",
        "                    batch_size = BATCH_SIZE,\n",
        "                    epochs= 3,\n",
        "                    verbose = 1,\n",
        "                    shuffle= True,\n",
        "                    callbacks=[callback],\n",
        "                       )"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3.9.13 ('base')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.9.13"
    },
    "vscode": {
      "interpreter": {
        "hash": "a7fe7439c5735edab5b1c40c746e0470d90449e584972202a3f74838ceba4319"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
