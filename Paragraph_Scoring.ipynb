import nltk
nltk.download('wordnet')
nltk.download('punkt')
nltk.download('omw-1.4')
nltk.download('averaged_perceptron_tagger')
nltk.download('stopwords')
from nltk.stem import WordNetLemmatizer
from nltk.corpus import stopwords
from nltk.tokenize import sent_tokenize,word_tokenize
from gensim.models import Word2Vec
import numpy as np
import re
import pandas as pd

def lemmatize_words(words): # used
    lemmatized_words = []
    for word in words:
        tempList = []
        for word2 in word:
            tempList.append(wordlemmatizer.lemmatize(word2))
        lemmatized_words.append(tempList)
    return lemmatized_words

def uniqueWord(w):
    w2=[]
    for word in w:
        tempList=[]
        for word2 in word:
            if tempList.count(word2)<1:
                    tempList.append(word2)
        w2.append(tempList)
    return w2

def remove_special_characters(text): # used
    regex = r'[^a-zA-Z0-9\s]'
    text = re.sub(regex,'',text)
    return text

def removeStopWord(word_text):  # used
    filtered_sentence = [] 
    stop_words = set(stopwords.words('english'))   
    for w in word_text:
        tempList=[]
        for x in w:
            if x.lower() not in stop_words: 
                tempList.append(x)
        filtered_sentence.append(tempList)
    return filtered_sentence   
 
def meanOfWord(model, sentence): # used
#     posValue=nltk.pos_tag(sentence)
    posList=['CD']
    nounList=['NN','NNP','NNS','NNPS']
    value=[]
    count=0
    noun=0
    for word in sentence:
        a=model.similar_by_word(word)
        temp=[]
        for w in a:
            temp.append(w[1])
        posValue=nltk.pos_tag([word])
#         print(posValue)
        wordScore=np.mean(temp)
        if posValue[0][1] in posList:
            count=count+1
        else:
            valueIfNum=checkNum(word)
            count=count+valueIfNum
        if posValue[0][1] in nounList:
            noun=noun + .4
        value.append(wordScore)
    return np.mean(value)+count+noun
    
def checkNum(s):
    l= ['1','2','3','4','5','6','7','8','9','0']
    check =False

    for i in s:
        if i in l:
            check = True
            break
    if check == True:
        return 1
    else:
        return 0
      
# lets begin ;) 
Stopwords = set(stopwords.words('english'))
wordlemmatizer = WordNetLemmatizer()
df = pd.read_csv("train.csv", usecols = ['full_text'])
text = df.iloc[9]['full_text']
sentences = sent_tokenize(text) # 1: sent tokenize
text_noSpecial_character = remove_special_characters(str(text)) # 2: remove special character:
word_text = [[text_noSpecial_character for text_noSpecial_character in sentences.split()] for sentences in sentences] # 3: word token
#print(word_text[0])
stop_text= removeStopWord(word_text) # 4: remove stop words
#print(stop_text[0])
unique_text= uniqueWord(stop_text)   # 5: remove duplicate words
#print(unique_text[0])
lemma_text = lemmatize_words(unique_text) # 6: lemmatization
#print(lemma_text[0])
model = Word2Vec(lemma_text, min_count=1,sg=1)
#print(model)
words = list(model.wv.vocab)
#print(words)
score=[]
mscore = []
for index, sentence in enumerate(lemma_text):
    i = lemma_text.index(sentence)
    meanScore= meanOfWord(model,sentence)
    print(sentence)
    print(meanScore)
#         print(str(labels[index])+ ":"+ str(sentence)+ str(meanScore) )
    temp = [i,meanScore]
    mscore.append(meanScore)
    score.append(temp)
#     print(meanOfWord(model,sentence))
print(sum(mscore)/len(mscore))
