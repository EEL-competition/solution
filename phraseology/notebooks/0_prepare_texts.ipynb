{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Requirements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "\n",
    "import language_tool_python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clean data of grammar mistakes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data\n",
    "data = pd.read_csv('../raw_data/train.csv')\n",
    "\n",
    "texts = data.loc[:, \"full_text\"].values\n",
    "scores = data.loc[:, \"phraseology\"].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  10.   11.  350.  772. 1153.  929.  553.  108.   25.]\n"
     ]
    }
   ],
   "source": [
    "analyze_scores = (scores*2-2).astype(int)\n",
    "b = np.zeros((analyze_scores.shape[0], analyze_scores.max() + 1))\n",
    "b[np.arange(analyze_scores.shape[0]), analyze_scores] = 1\n",
    "\n",
    "analyze_scores = np.sum(b, axis=0)\n",
    "\n",
    "print(analyze_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clear grammar errors - takes time\n",
    "tool = language_tool_python.LanguageTool('en-US')\n",
    "\n",
    "clear_texts = texts.copy()\n",
    "for i in range(texts.shape[0]):\n",
    "    clear_texts[i] = tool.correct(texts[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pavelpopov/.pyenv/versions/3.7.12/lib/python3.7/site-packages/ipykernel_launcher.py:8: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "# remove '.' and ',' symbols, cast to lowercase, split into words\n",
    "\n",
    "data = []\n",
    "for i in range(clear_texts.shape[0]):\n",
    "    # data.extend(np.array([clear_texts[i].replace('.', '').replace(',', '').lower().split()]))\n",
    "    data += [clear_texts[i].replace('.', '').replace(',', '').lower().split()]\n",
    "\n",
    "data = np.array(data) # data is numpy.ndarray of lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save clean data\n",
    "np.savez(\n",
    "    \"../preprocessed_data/phraseology.npz\",\n",
    "    texts=texts,\n",
    "    clear_texts=clear_texts,\n",
    "    data=data,\n",
    "    scores=scores,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data\n",
    "np_data = np.load(\"../preprocessed_data/phraseology.npz\", allow_pickle=True)\n",
    "data = np_data['data']\n",
    "scores = np_data['scores']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenize data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data\n",
    "np_data = np.load(\"../preprocessed_data/phraseology.npz\", allow_pickle=True)\n",
    "texts = np_data['clear_texts']\n",
    "scores = np_data['scores']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I think that students would benefit from learning at home, because they won't have to change and get\n",
      "\n",
      "\n",
      "When a problem is a change you have to let it do the best on you no matter what is happening it can \n"
     ]
    }
   ],
   "source": [
    "print(texts[0][0:100])\n",
    "print()\n",
    "print()\n",
    "print(texts[1][0:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', \n",
    "                                          do_lower_case=True)\n",
    "\n",
    "encoded_data = tokenizer.batch_encode_plus(\n",
    "    texts, \n",
    "    add_special_tokens=True, \n",
    "    return_attention_mask=True, \n",
    "    # padding='max_length',\n",
    "    pad_to_max_length=True, \n",
    "    max_length=512, \n",
    "    return_tensors='np'\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['input_ids', 'token_type_ids', 'attention_mask'])\n",
      "\n",
      "<class 'numpy.ndarray'>\n",
      "(3911, 512)\n",
      "[ 101 1045 2228 2008 2493 2052 5770 2013 4083 2012]\n",
      "[ 101 2043 1037 3291 2003 1037 2689 2017 2031 2000]\n",
      "\n",
      "<class 'numpy.ndarray'>\n",
      "(3911, 512)\n",
      "[0 0 0 0 0 0 0 0 0 0]\n",
      "[0 0 0 0 0 0 0 0 0 0]\n",
      "\n",
      "<class 'numpy.ndarray'>\n",
      "(3911, 512)\n",
      "[1 1 1 1 1 1 1 1 1 1]\n",
      "[1 1 1 1 1 1 1 1 1 1]\n"
     ]
    }
   ],
   "source": [
    "print(encoded_data.keys())\n",
    "\n",
    "print()\n",
    "print(type(encoded_data[\"input_ids\"]))\n",
    "print(encoded_data[\"input_ids\"].shape)\n",
    "print(encoded_data[\"input_ids\"][0][0:10])\n",
    "print(encoded_data[\"input_ids\"][1][0:10])\n",
    "\n",
    "print()\n",
    "print(type(encoded_data[\"token_type_ids\"]))\n",
    "print(encoded_data[\"token_type_ids\"].shape)\n",
    "print(encoded_data[\"token_type_ids\"][0][0:10])\n",
    "print(encoded_data[\"token_type_ids\"][1][0:10])\n",
    "# print(encoded_data[\"token_type_ids\"][0])\n",
    "\n",
    "print()\n",
    "print(type(encoded_data[\"attention_mask\"]))\n",
    "print(encoded_data[\"attention_mask\"].shape)\n",
    "print(encoded_data[\"attention_mask\"][0][0:10])\n",
    "print(encoded_data[\"attention_mask\"][1][0:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save tokenized data\n",
    "np.savez(\n",
    "    \"../preprocessed_data/tokenized.npz\",\n",
    "    input_ids=encoded_data[\"input_ids\"],\n",
    "    token_type_ids=encoded_data[\"token_type_ids\"],\n",
    "    attention_mask=encoded_data[\"attention_mask\"],\n",
    "    scores=scores,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np_data = np.load(\"../preprocessed_data/tokenized.npz\", allow_pickle=True)\n",
    "data = {\n",
    "    \"input_ids\": np_data[\"input_ids\"],\n",
    "    \"token_type_ids\": np_data[\"token_type_ids\"],\n",
    "    \"attention_mask\": np_data[\"attention_mask\"],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data[\"input_ids\"][0][0:10])\n",
    "print(data[\"input_ids\"][1][0:10])\n",
    "\n",
    "\n",
    "print(data[\"token_type_ids\"][0][0:10])\n",
    "print(data[\"token_type_ids\"][1][0:10])\n",
    "\n",
    "\n",
    "print(data[\"attention_mask\"][0][0:10])\n",
    "print(data[\"attention_mask\"][1][0:10])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.12 64-bit ('3.7.12')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "1b99d877adb0b7ab3fa56a43902a76f28d27732ee5cf08679707ced7dcdfd859"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
